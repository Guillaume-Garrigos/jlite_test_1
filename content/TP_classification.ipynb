{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TP : Modélisation et résolution d'un problème de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions générales:**\n",
    "\n",
    "- Je vous conseille de réaliser ce devoir avec `jupyter lab` et non pas `jupyter notebook`. La raison est que `jupyter notebook` a parfois du mal à afficher des images.\n",
    "- Les dépendances nécessaires sont incluses dans le fichier `requirements.txt`.\n",
    "- Le devoir vous a été distribué sous la forme d'un dossier contenant un notebook et d'autres fichiers. Vous pouvez déplacer le dossier où vous voulez dans votre ordinateur, mais ne changez pas la structure de ce dossier : le notebook a besoin de ces fichiers auxiliaires pour fonctionner!\n",
    "- Certaines questions nécessitent de faire tourner un algorithme. Je vous recommande de ne pas faire tourner vos algorithmes plus qu'une trentaine de secondes (chez moi tout marche bien en moins d'une seconde)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description du devoir:**\n",
    "\n",
    "- la partie **I** est une mise en bouche avec le problème.\n",
    "- la partie **II** porte sur un problème-jouet, et est celle où vous mettrez en valeur certaines des choses que vous avez appris en cours. Elle se décompose en trois sections **indépendantes**, qui proposent de résoudre le problème de trois manières différentes. Elles peuvent être traitées à des moments différents du cours:\n",
    "  - la partie **II.A** ne requiert aucune connaissance particulière du cours. Si vous voulez simplement vous amuser avec le sujet c'est par là!\n",
    "  - la partie **II.B** porte sur la dualité de Rockafellar (fin du chapitre 2). Elle vous demandera de faire des petits calculs de dualité à la main. C'est l'occasion de voir si vous avez compris les bases de ce chapitre!\n",
    "  - la partie **II.C** porte sur les méthodes d'éclatement, et nécéssite d'avoir vu la notion d'opérateur proximal (début du chapitre 3).\n",
    "- la partie **III** est une application à un problème de traitement de données, qui consiste à exploiter les résultats de la partie **II** sur un vrai problème. C'est là que vous montrerez si vous avez compris ce qui se passe dans la partie **II**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cette cellule importe les modules nécéssaires à ce TP. En cas d'erreur, vous devrez au préalable installer les modules manquants.\n",
    "# Lors d'une première exécution cela peut prendre un peu de temps, jusqu'à 1 min parfois :(\n",
    "import numpy\n",
    "import matplotlib\n",
    "import scipy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je vous invite pour commencer à aller lire la section **\"Modélisation : classification\"** qui se trouve en annexe du [polycopié](https://cloud.math.univ-paris-diderot.fr/s/fLXPipwgSC9tpzj) du cours. Il y est décrit ce qu'est un problème de classification, et comment le modéliser mathématiquement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Premier contact avec le problème"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** On commence par importer des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('data/donnees_entrainement.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces données correspondent à $m$ points de $\\mathbb{R}^2$, qui sont rangées dans une matrice à $m$ lignes et 2 colonnes.\n",
    "Chaque *ligne* de la matrice correspond donc à un point de $\\mathbb{R}^2$.\n",
    "Vous pouvez regarder quelle est la taille de la matrice avec la méthode `X.shape`, ce qui vous permettra de déterminer la valuer de $m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 200 # à definir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vu que nos données sont des points du plan, on va pouvoir facilement les visualiser avec la fonction `plt.scatter` de pyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couleur = [None]*m # liste vide de taille m\n",
    "for k in range(m):\n",
    "    couleur[k] = 'blue'\n",
    "_=plt.scatter(X[:, 0], X[:, 1], c=couleur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Vous devriez être maintenant convaincus que ce jeu de données contient deux groupes de points appartenant à des familles distinctes. Or nous n'en savons rien, à priori, le seul moyen d'en être sur est d'aller regarder les *etiquettes* correspondantes. Importons-les:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.load('data/etiquettes_entrainement.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifiez que ce vecteur ne contient que des étiquettes $\\pm 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En adaptant le code de la précédente question, affichez de nouveau ce nuage de points cette fois-ci avec deux couleurs: rouge pour les points avec l'étiquette $-1$, et bleu pour les points avec l'étiquette $+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couleur = [None]*m\n",
    "for k in range(m):\n",
    "    if Y[k] == 1:\n",
    "        couleur[k] = 'blue'\n",
    "    else:\n",
    "        couleur[k] = 'red'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.scatter(X[:, 0], X[:, 1], c=couleur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Une approche intuitive pour classer ce problème consiste à tracer une droite qui va séparer ces deux nuages: ainsi fait, on pourra dire que tout nouveau point qui apparaitra d'un côté de la droite sera \"rouge\" et de l'autre les \"bleus\".\n",
    "\n",
    "Ici on va considérer une droite dans $\\mathbb{R}^2$ comme étant l'ensemble des points satisfaisant l'équation\n",
    "\n",
    "$$ D = \\{ x \\in \\mathbb{R}^2 \\ | \\ \\langle a, x \\rangle = b \\},$$\n",
    "\n",
    "où $a \\in \\mathbb{R}^2$ et $b \\in \\mathbb{R}$ sont à choisir. On notera par la suite $w = (a_1, a_2 ,b) \\in \\mathbb{R}^2 \\times \\mathbb{R}$ le vecteur de paramètres décrivant la droite $D$.\n",
    "\n",
    "En utilisant le code ci-dessous, ajoutez le tracé d'une droite aux données, et jouez avec la valeur de $w$ pour essayer de trouver la droite qui sépare le mieux les deux classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_droite(w):\n",
    "    # trace la droite des points x vérfiant l'équation <a,x>=b\n",
    "    # où w = (a_1, a_2, b)\n",
    "    # on va la tracer comme la droite alpha * t + b\n",
    "    a = w[0:2]\n",
    "    b = w[2]\n",
    "    if a[1] == 0:\n",
    "        if a[0] != 0:\n",
    "            # on a une droite verticale\n",
    "            x = b/a[0]\n",
    "            plt.axvline(x=x)\n",
    "        else: \n",
    "            return\n",
    "            # a=0 donc cela ne définit pas une droite, on ne trace rien\n",
    "    else:\n",
    "        alpha = -a[0]/a[1]\n",
    "        beta = b/a[1]\n",
    "        abscisse = np.arange(-5,5,0.1)\n",
    "        ordonnee = alpha*abscisse + beta\n",
    "        plt.plot(abscisse, ordonnee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0, 1, 0]) # à modifier\n",
    "trace_droite(w)\n",
    "_=plt.scatter(X[:, 0], X[:, 1], c=couleur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pensez-vous que certaines droites sont meilleures que les autres?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Modélisation : trouver une droite séparatrice optimale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je vous invite si ce n'est toujours pas fait à aller lire la section **\"Modélisation : classification\"** qui se trouve en annexe du [polycopié](https://cloud.math.univ-paris-diderot.fr/s/fLXPipwgSC9tpzj) du cours.\n",
    "Il y est expliqué comment faire pour trouver un hyperplan qui sépare bien des données dans $\\mathbb{R}^n$, et plus précisément comment trouver le *meilleur* hyperplan séparateur possible (en un certain sens).\n",
    "\n",
    "Plus précisément, considérons le problème de séparer $m$ points $x_i \\in \\mathbb{R}^n$ avec étiquettes $y_i \\in \\{\\pm 1\\}$.\n",
    "Alors on peut montrer que le meilleur hyperplan séparateur est défini par\n",
    "\n",
    "$$ \\mathcal{H}_w := \\{ x \\in \\mathbb{R}^n \\ | \\ \\langle a,x \\rangle = b \\}, \\quad w=(a,b) \\in \\mathbb{R}^n \\times \\mathbb{R},$$\n",
    "\n",
    "où $w=(a,b) \\in \\mathbb{R}^n \\times \\mathbb{R}$ est la solution du problème d'optimisation quadratique convexe suivant (appelé SVM) : \n",
    "\\begin{equation}\n",
    "    \\min\\limits_{w \\in \\mathbb{R}^{n} \\times \\mathbb{R}} \\ \\frac{1}{2} \\Vert Pw \\Vert^2 \n",
    "    \\quad \n",
    "    \\text{ sous la contrainte que } \\Phi w \\preceq -e,\n",
    "    \\tag{P}\n",
    "\\end{equation}\n",
    "où \n",
    "\n",
    "- $e:=(1,\\dots,1)^\\top \\in \\mathbb{R}^m$ \n",
    "- $P \\in \\mathcal{M}_{n+1}(\\mathbb{R})$ est la matrice de projection sur $F = \\{ w \\in \\mathbb{R}^{n+1} \\ | \\ w_{n+1}=0\\}$. Autrement dit $P$ projette sur les $n$ premières coordonnées. On se convainc que $P= {\\rm{diag}}(1, \\dots, 1, 0)$.\n",
    "- $\\Phi \\in \\mathcal{M}_{m,n+1}(\\mathbb{R})$ est la matrice définie par\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\Phi :=\n",
    "    \\begin{pmatrix}\n",
    "        -y_1 x_1^\\top & y_1 \\\\\n",
    "        \\vdots & \\vdots \\\\\n",
    "        -y_i x_i^\\top & y_i \\\\\n",
    "        \\vdots & \\vdots \\\\\n",
    "        -y_m x_m^\\top & y_m\n",
    "    \\end{pmatrix}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# II. Résolution du problème : phase d'entraînement\n",
    "\n",
    "Dans cette partie, nous allons résoudre le problème (P) de trois manières différentes, dans les sections A,B et C.\n",
    "Ces trois sections sont de difficulté variable, mais indépendantes:\n",
    "\n",
    "- la section A ne requiert aucun prérequis\n",
    "- la section B requiert d'avoir vu la dualité de Fenchel-Rockafellar. Elle contient notamment des petits exercices que vous êtes encouragés à faire.\n",
    "- la section C requiert d'avoir vu la notion d'opérateur proximal. Elle applique un algorithme vu dans le dernier chapitre du cours.\n",
    "\n",
    "Le but est d'écrire du code pour des données qui vivent en dimension $n$, et de les appliquer dans un premier temps à nos nuages de points pour lesquels $n=2$.\n",
    "Dans la partie suivante nous appliquerons ce code à un vrai problème plus intéressant, pour lequel $n >> 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0)** On définit `e`, le vecteur $e:=(1,\\dots,1)^\\top \\in \\mathbb{R}^m$, ainsi que `Phi`, la matrice $\\Phi \\in \\mathcal{M}_{m,3}(\\mathbb{R})$ définie dans la section précédente. On rappelle que cette matrice ne dépend que de `X` et `Y`.\n",
    "\n",
    "On définira également `infini`, le vecteur $(+\\infty, \\dots, +\\infty) \\in \\mathbb{R}^m$, sachant que la quantité $+\\infty$ s'obtient avec `np.inf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e=np.ones(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = np.zeros((m,3)) # matrice vide\n",
    "Phi[:, 0:-1] = -X # toutes les colonnes sauf une remplies par -X\n",
    "Phi[:, -1] = e # dernière colonne remplie par des 1\n",
    "Phi = np.diag(Y)@Phi # on multiplie chaque ligne par y_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infini = e*np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A. Résolution via un solveur Python\n",
    "\n",
    "Ici nous allons simplement donner le problème à Python, qui dispose de certaines routines pour résoudre les problèmes d'optimisation. Toute la difficulté consiste en la mise en forme du problème dans une syntaxe que Python peut comprendre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Définir une fonction `objectif` qui prend en entrée un vecteur $w=(a,b) \\in \\mathbb{R}^n \\times \\mathbb{R}$ et qui renvoie $\\Vert a \\Vert^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** On doit résoudre un problème d'optimisation sous contrainte. Pour cela, on va faire appel à la bibliothèque `scipy.optimize` qui peut faire cela pour nous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import LinearConstraint, minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord on doit définir la contrainte $\\Phi w \\preceq -e$. Il faut pour cela utiliser la fonction `LinearConstraint` qui permet de définir une contrainte de la forme\n",
    "$$\n",
    "\\{ w \\in \\mathbb{R}^d \\ | \\ a \\preceq \\Phi w \\preceq b \\}\n",
    "$$\n",
    "avec la commande `LinearConstraint(Phi, a, b)` (voir la [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.LinearConstraint.html#scipy.optimize.LinearConstraint) pour plus de détails). A vous de compléter la commande suivante, avec les objets définis à la question **1)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrainte = LinearConstraint(############)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** On peut maintenant obtenir la solution de notre problème, en faisant appel à `scipy.optimize.minimize` (voir la [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) pour plus de détails)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sol_scipy = minimize(                          # minimiser\n",
    "                    fun = objectif,           # la fonction 'objectif'\n",
    "                    constraints = contrainte, # sous la contrainte 'contrainte'\n",
    "                    x0 = np.random.randn(3),  # en partant d'un point initial\n",
    "                    ).x                       # et donne-moi le vecteur solution\n",
    "w_sol_scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** Reprendre le code de la partie **I.1**, pour afficher les nuages de points ainsi que la droite définie par le paramètre $w_{sol-scipy} \\in \\mathbb{R}^3$ que vous venez d'obtenir. Êtes-vous satisfait(e) de la solution obtenue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_droite(w_sol_scipy)\n",
    "_=plt.scatter(X[:, 0], X[:, 1], c=couleur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## B. Résolution via le problème dual et la méthode du gradient projeté"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous allons nous intérésser à (D), le problème dual de (P) au sens de Fenchel-Rockafellar. Dans un premier temps nous allons calculer ce problème dual. Dans un second temps, nous allons voir qu'il a une structure plus simple, le rendant facile à résoudre.\n",
    "Dans un troisième temps, nous allons résoudre (D) avec la méthode du gradient projeté, puis retrouver une solution du problème primal (P)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Calcul du problème dual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** 📝 Montrer que le problème (P) peut s'écrire sous la forme \n",
    "\\begin{equation*}\n",
    "    \\min\\limits_{w \\in \\mathbb{R}^{n+1}} \\ f(w) + g(\\Phi w), \n",
    "    \\tag{P}\n",
    "\\end{equation*}\n",
    "où\n",
    "- $f$ est une fonction quadratique semi définie positive,\n",
    "- $g = \\delta_C$ est la fonction indicatrice de $C$, un ensemble convexe fermé non vide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** 📝 D'après la dualité de Fenchel-Rockafellar, le problème dual de (P) consiste à résoudre\n",
    "\\begin{equation*}\n",
    "    \\min\\limits_{u \\in \\mathbb{R}^m} \\ f^* (-\\Phi^\\top u) + g^* (u), \n",
    "    \\tag{D}\n",
    "\\end{equation*}\n",
    "Vous devrez:\n",
    "\n",
    "- Calculer la conjuguée de Fenchel $f^*$. Pour cela vous pourrez utiliser un résultat de la feuille de TD sur la conjuguée.\n",
    "- Calculer la conjuguée de Fenchel $g^*$. Vous utiliserez des résultats du cours  pour montrer que \n",
    "\\begin{equation*}\n",
    "    g^*(u) = \\delta_K(u) - \\langle e, u \\rangle,\n",
    "\\end{equation*}\n",
    "où $K$ est un cône convexe fermé (très sympatique) non vide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** 📝 Conclure que le problème dual s'écrit\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\min\\limits_{u \\in \\mathbb{R}^M} \\ h(u), \n",
    "    \\quad \n",
    "    \\text{ sous la contrainte que } u \\in K \\text{ et } \\langle y,u \\rangle =0,\n",
    "    \\tag{D}\n",
    "\\end{equation*}\n",
    "où $h$ est une fonction convexe quadratique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Définition de la projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Ici on s'intéresse à la projection sur la contrainte duale\n",
    "$$D:= \\{ u \\in \\mathbb{R}^M \\ | \\ u \\in K \\text{ et } \\langle y,u \\rangle =0\\},$$\n",
    "où $K$ est le cône simple que vous avez obtenu à la section précédente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** On va ici implémenter une alternative à la méthode de projection approchée de la question précédente avec la **projection de Kiwiel**. Cette dernière s'effectue avec la fonction `proj_kiwiel` (importée ci-dessous), qui:\n",
    "\n",
    "- projette un vecteur $u$ sur une intersection $K \\cap H$ \n",
    "   - où $H$ est un hyperplan $H = \\{ u \\in \\mathbb{R}^M \\ | \\ \\langle y, u \\rangle =0 \\}$\n",
    "   - où $K$ est un ensemble défini par $K = \\{ u \\in \\mathbb{R}^M \\ | \\ a \\preceq u \\preceq b \\}$\n",
    "     - ici les bornes $a, b \\in \\mathbb{R}^M$ peuvent prendre des valeurs $\\pm \\infty$\n",
    "- prend en entrée `proj_kiwiel(u, y, a, b)` où $u$ est le vecteur à projeter, $y$ le vecteur définissant l'hyperplan $H$, et $a,b$ sont les bornes (inférieures et supérieures) définissant $K$.\n",
    "- renvoie en sortie la projection de $u$ sur $K \\cap H$.\n",
    "\n",
    "Votre travail ici consiste à définir une fonction `proj_D` qui prend en entrée un vecteur `u`, le vecteur `y`, et renvoie la projection de $u$ sur $D$.\n",
    "Pour ce faire, vous définirez les bornes `a` et `b` qui définissent $K$, et vous appliquerez la projection de Kiwiel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.kiwiel import proj_kiwiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_D(u, y):\n",
    "    # ########  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) BONUS:** Dans cette question totalement facultative, on vous propose de coder vous-même une projection sur $K \\cap H$ en utilisant les algorithmes vus en cours. \n",
    "En effet, on peut écrire\n",
    "\n",
    "$$ proj_{K \\cap H}(x) = {\\rm{argmin}}_y  \\frac{1}{2} \\Vert y - x \\Vert^2 + \\delta_K(y) + \\delta_H(y).$$\n",
    "\n",
    "Il s'agit donc de résoudre un problème de minimisation de la forme $f + g$ où $f(y) = \\frac{1}{2} \\Vert y - x \\Vert^2 + \\delta_K(y)$ et $g(y) = \\delta_H(y)$.\n",
    "Ce type de problème peut se résoudre avec l'algorithme de Douglas-Rachford, qui dans ce cas particulier devient (avec un peu de travail) la méthode de **projection de Dijkstra**:\n",
    "Elle se définit via des projections alternées sur $H$ et $K$, comme suit:\n",
    "\n",
    "| Projection de Dijkstra |\n",
    "|-|\n",
    "| On veut projeter $u$ sur $K\\cap H$ |\n",
    "| On initialise $x_0=u$, et $h_0=p_0=q_0=0$, puis pour $k \\geq 0$: | \n",
    "| $$\\begin{cases} h_{k+1} & =  {\\rm{proj}}_H(x_k+p_k) \\\\ p_{k+1} &= x_k + p_k - h_{k+1} \\\\ x_{k+1} &= {\\rm{proj}}_K( h_{k+1} + q_k) \\\\ q_{k+1} &= h_{k+1} + q_k - x_{k+1} \\end{cases}$$ |\n",
    "| **Théorème:** $x_k \\to {\\rm{proj}}_{K\\cap H}(u)$ lorsque $k \\to +\\infty$ | \n",
    "\n",
    "Nous vous proposer de coder la projection sur $K$ et $H$ (qui sont relativement faciles), puis de calculer une dizaine d'itérations de la projection de Dijkstra. Vous pourrez alors comparer votre résultat avec la projection de Kiwiel (en projetant un vecteur aléatoire par exemple), et avoir le plaisir d'avoir calculé cette projection vous-même."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Méthode du gradient projeté"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode du gradient projeté permet de résoudre un problème de la forme \n",
    "$$ \\min f(x), x \\in C$$\n",
    "L'algorithme s'écrit \n",
    "\n",
    "| | | |\n",
    "|-|-|-|\n",
    "|| On choisit $x_0$ un vecteur de $\\mathbb{R}^N$ et $\\rho > 0$ un pas fixe. | \n",
    "|  | Pour $k\\geq 0$ : $\\qquad \\qquad \\, $   $x_{k+1}$  = ${\\rm{proj}}_C(x_k - \\rho \\nabla f(x_k))$  | \n",
    "\n",
    "L'algorithme est garanti de converger si $f$ est convexe, de classe $C^1$, avec un gradient $L$-Lipschitzien, et un pas $\\rho \\in ]0,2/L[$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** 📝 Vérifier que les hypothèses ci-dessus sont vérifiées pour le problème dual (D). En particulier, vous calculerez $\\nabla h$, et vous définirez sa constante de Lipschitz `L`.\n",
    "On rappelle que la norme d'opérateur d'une matrice `A` s'obtient avec `np.linalg.norm(A,2)` ; et que la constante de Lipschitz de $\\nabla f$ est la borne supérieure de $\\Vert \\nabla^2 f(x) \\Vert$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Définir une fonction `FB_dual` qui résoud le problème dual (D) en:\n",
    "\n",
    "- prenant en entrée la matrice `Phi`, le vecteur `y`, un pas `rho`, un nombre d'itérations `nb_iter`\n",
    "- résolvant le problème dual en appliquant la méthode du gradient projeté :\n",
    "  - en initialisant les itérés en $0 \\in \\mathbb{R}^m$\n",
    "  - avec un pas `rho`\n",
    "  - en réalisant la projection sur $D$ avec une fonction définie dans la section précédente (de votre choix)\n",
    "  - pendant un nombre d'itérations égal à `nb_iter`\n",
    "- retournant en sortie le dernier itéré calculé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FB_dual(Phi, y, rho, nb_iter=500):\n",
    "    # #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Trouver `u_sol_GPD` une solution approchée au problème dual (D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** 📝 D'après la caractérisation des solutions du problème primal-dual vu en cours, on sait que la solution $w$ du problème primal est liée à la solution $u$ du problème dual via l'inclusion $w \\in \\partial f^*(-\\Phi^\\top u)$.\n",
    "Montrer que ceci implique que $-\\Phi^\\top u$ peut s'écrire   $-\\Phi^\\top u= (a,0)$ où $a \\in \\mathbb{R}^n$, et que $w=(a,b)$ pour un certain $b$ à déterminer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**5)** Vous vérifierez numériquement que la dernière coordonnée de $\\Phi^\\top u$ est bien nulle. Définissez `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)** On admet par la suite que l'on peut prendre $b=0$, c-à-d que $w = -\\Phi^\\top u$ est solution de notre problème primal (P). Reprendre le code de la partie **I.1**, pour afficher les nuages de points ainsi que la droite définie par le paramètre $w \\in \\mathbb{R}^3$ que vous venez d'obtenir. Êtes-vous satisfait(e) de la solution obtenue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Résolution via une méthode d'éclatement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section, l'objectif est que vous soyez plus autonomes. Nous vous fournissons un algorithme, un théorème, et vous devez vous en servir pour trouver une solution aux problèmes (P) et (D). A vous de vérifier que votre code fonctionne et vous fournit une solution satisfaisante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On introduit l'algorithme de **Loris-Verhoeven**, qui est une généralisation de la méthode du gradient proximal, et un cas particulier de l'algorithme de Yan, qui s'applique à la minimisation d'une fonction de la forme $g(Ax) + h(x)$, où $g \\in \\Gamma_0(\\mathbb{R}^M)$ et $h \\in \\Gamma_0(\\mathbb{R}^N) \\cap C^{1,1}_L(\\mathbb{R}^N)$:\n",
    "\n",
    "| L'algorithme de **Loris-Verhoeven** |\n",
    "| --- |\n",
    "| Soient $x_0 \\in \\mathbb{R}^N, u \\in \\mathbb{R}^M$, $\\lambda, \\sigma >0$ |\n",
    "| $$\\begin{cases} x_{k+1} &= x_k - \\lambda \\nabla h(x_k) - \\lambda A^\\top u_n \\\\ u_{k+1} &=  {\\rm prox}_{{\\sigma}g^*} \\left(u_n + \\sigma A \\left[2x_{k+1} - x_k + \\lambda \\nabla h(x_k) - \\lambda \\nabla h(x_{k+1}) \\right]\\right) \\end{cases}$$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Théorème:** Soient $h : \\mathbb{R}^n \\to \\mathbb{R}$ une fonction différentiable à gradient $L$-Lipschitzien,  $g : \\mathbb{R}^m \\to \\mathbb{R}\\cup\\{+\\infty\\}$ une fonction convexe s.c.i. propre, et $A \\in \\mathcal{M}_{m,n}(\\mathbb{R})$.\n",
    "Soit $(x_k,u_k)_{k \\in \\mathbb{N}}$ une suite générée par l'algorithme de Loris-Verhoeven, avec\n",
    "$$ \\lambda \\in \\left]0, \\frac{2}{L}\\right[ \\quad \\text{ et } \\quad \\sigma \\in \\left]0, \\frac{1}{\\lambda \\Vert A \\Vert^2}\\right].$$\n",
    "Alors \n",
    "- $x_k$ converge vers un minimiseur de $f + g \\circ A$, s'il en existe,\n",
    "- $u_k$ converge vers un minimiseur de $f^* \\circ -A^\\top + g^*$ s'il en existe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparer les trois solutions obtenues. D'une part vous visualiserez les 3 droites séparatrices correspondantes. Ensuite vous évaluerez $f$ en ces points. Donner votre avis sur vos résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# III. Classifier des images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Obtention des données : Importer le jeu de données MNIST\n",
    "\n",
    "On va résoudre un problème similaire, sauf que cette fois les données ne sont plus des points $x_i \\in \\mathbb{R}^2$ rouges ou bleus, mais des images $x_i \\in \\mathbb{R}^{64}$ de chiffres écrits à la main.\n",
    "On ne parlera donc plus de \"droite séparatrice\" mais d'\"hyperplan séparateur\".\n",
    "\n",
    "| Un nouveau jeu de données | \n",
    "| ----------- |\n",
    "| ![](images/mnist.jpg) |\n",
    "| Chaque image de $8\\times 8$ pixels est représentée par un point $x_i \\in \\mathbb{R}^{64}$ |\n",
    "\n",
    "Pour ce TP on va se contenter de séparer des images de 0 et de 1. Leurs étiquettes $y_i$ prendront respectivement les valeurs $-1$ et $+1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres pour l'affichage des images de nombres\n",
    "plt.rcParams['xtick.bottom'] = False\n",
    "plt.rcParams['ytick.left'] = False\n",
    "plt.rcParams['xtick.labelbottom'] = False\n",
    "plt.rcParams['ytick.labelleft'] = False\n",
    "plt.rcParams['image.cmap'] = 'gray_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importe les données\n",
    "digits = sklearn.datasets.load_digits() # Importe un jeu de données à classer (10 classes)\n",
    "digits.data = digits.data*1/np.max(digits.data) # normalise : coeffs dans [0,1]\n",
    "classes = [0, 1] # Définit les 2 classes de nombres avec lesquelles on va travailler\n",
    "idx_classes = np.logical_or(digits.target == classes[0], digits.target == classes[1]) # Localise les deux classes ..\n",
    "digits.data = digits.data[idx_classes] # .. les extrait ..\n",
    "digits.target = digits.target[idx_classes] # .. et jette le reste\n",
    "digits.target = np.where(digits.target==classes[0],-1, 1) # Transforme les étiquettes de classes en {-1,+1}\n",
    "\n",
    "X, X_test, Y, Y_test = sklearn.model_selection.train_test_split( # On coupe le jeu de données en deux\n",
    "                        digits.data, digits.target, train_size=25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affiche les 25 premières images de X\n",
    "fig, axs = plt.subplots(5, 5, figsize=(3, 3))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        k = i*5 + j\n",
    "        _ = axs[i,j].imshow(X[k].reshape(8,8))\n",
    "plt.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** On dispose de données `X`, qui contient des images de 0 et 1 écrits à la main. Plus précisément, pour tout `k`, `X[k]` représente une telle image 2D (8x8 pixels) qui a été aplatie en un vecteur 1D.\n",
    "\n",
    "Utiliser `.shape` pour déterminer le nombre d'images que contient `X`.\n",
    "\n",
    "Prendre une image au hasard, et l'afficher avec la fonction `plt.imshow()`. Afin de l'afficher, vous aurez besoin de temporairement remettre cette image sous forme 2D avec la méthode `.reshape(nb_lignes, nb_colonnes)` . Est-ce l'image d'un 0 ou d'un 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[10].reshape(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** On dispose d'étiquettes `Y` qui encodent la nature des images contenues dans `X`. Plus précisément, `Y[k]` contient `-1` si `X[k]` est l'image d'un 0, ou `+1` si `X[k]` est l'image d'un 1.\n",
    "\n",
    "Reconsidérer l'image affichée à la question précédente, et vérifier que son étiquette correspond à ce que vous avez observé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Phase d'entraînement : Trouver un classifieur avec la méthode SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0)** Définir `m` le nombre de données dans notre jeu de données ; et `n` la taille de chacune de ces données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[0]\n",
    "n = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Nous allons trouver un classifieur linéaire avec la méthode SVM, comme au I.\n",
    "\n",
    "Pour ce faire, définir deux vecteurs `e`$= (1, \\dots , 1)^\\top$, `infini`$=(+\\infty, \\dots, +\\infty)^\\top$ de $\\mathbb{R}^m$, ainsi que la matrice $\\Phi \\in \\mathcal{M}_{m,n+1}(\\mathbb{R})$ définie en Section I.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.ones(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = np.zeros((m, n+1)) # matrice vide\n",
    "Phi[:, 0:-1] = -X # toutes les colonnes sauf une remplies par -X\n",
    "Phi[:, -1] = e # dernière colonne remplie par des 1\n",
    "Phi = np.diag(Y)@Phi # on multiplie chaque ligne par y_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infini = e*np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Adapter à partir de la Section **II** la méthode de résolution de votre choix pour obtenir une solution $w=(a,b) \\in \\mathbb{R}^{n+1}$ du problème de SVM (P). Vous appellerez cette solution `w_mnist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Ecrire une fonction `classifieur`, qui:\n",
    "- prend en entrée une image aplatie `x`$\\in \\mathbb{R}^n$\n",
    "- prend en entrée un vecteur de paramètres  `w`$=(a,b)\\in \\mathbb{R}^n \\times \\mathbb{R}$\n",
    "- retourne +1 si $\\langle a, x \\rangle \\geq  b$, -1 si $\\langle a, x \\rangle \\leq b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifieur(x, w):\n",
    "    # ###########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** Prendre une image au hasard dans `X`, et comparer sa vraie étiquette avec la prédiction faite par notre nouveau classifieur. Notre prédiction est elle bonne?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Phase de test : connaitre la vraie performance de notre classifieur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vient de voir que notre classifieur marche bien lorsque on l'applique aux images contenues dans `X`. Or ceci n'est pas très surprenant : le classifieur a été construit à partir de `w_mnist`, la solution d'un problème d'optimisation dépendant des données contenues dans `X,Y`. Pour vraiment déterminer si notre modèle a **appris** quelque chose, il faut le tester sur des données qu'il n'a encore **jamais vues**.\n",
    "\n",
    "On va donc maintenant utiliser les données de test `X_test` et `Y_test` que l'on a pas encore utilisées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Calculer le pourcentage de bonnes réponses données par notre `classifieur`. Autrement dit, vous allez parcourir l'ensemble des données de test, et calculer le pourcentage du nombre de fois que le classifieur donne la bonne prédiction, en la comparant à la vraie étiquette contenue dans `Y_test`. Que pensez-vous du nombre obtenu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Si le taux de bonne réponse n'est pas de 100%, essayez de trouver dans le jeu de données quelles sont les images sur lesquelles le classifieur se trompe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Si vous retournez au début de la section **III.**, vous pouvez voir qu'au moment de l'importation nous avons demandé à travailler avec les `classes` 0 et 1.\n",
    "\n",
    "Remplacez ces chiffres par deux autres chiffres de votre choix, et relancez votre code afin de déterminer le taux de bonne réponse du classifieur. Essayez de prendre des chiffres difficiles à classer!\n",
    "\n",
    "**NB:** Vous ne serez pas notés sur cette question, qui vous laisse libre de vous amuser. Néanmois je vous conseille de ne pas l'ignorer, car vous devrez de toute façon faire ce genre de choses dans les questions suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Aller plus loin : Classification multiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "On a vu comment classer des données à deux étiquettes. Mais en pratique il y a souvent un plus grand nombre de classes : par exemple MNIST peut contenir jusqu'à 10 classes: les chiffres de 0 à 9! Un tel classifieur vous est par exemple mis à disposition sur [ce site](https://mco-mnist-draw-rwpxka3zaa-ue.a.run.app/), qui vous permet de dessiner en ligne un chiffre et vous fournira en temps réel une estimation de la probabilité d'appartenance à une classe de chiffre.\n",
    "\n",
    "Je vous propose dans cette section de construire un tel classifieur. Notre stratégie va consister à réunir plusieurs classifieurs à deux classes (que vous avez appris à construire dans la section précédente) pour construire un classifieur à 10 classes. \n",
    "Plus précisément notre stratégie sera:\n",
    "\n",
    "- Couper le jeu de données en deux classes : les 0 et le reste (1, ..., 9). On produit alors un classifieur qui sera capable de savoir si une image est un zéro, ou non.\n",
    "- On refait la même chose en isolant cette fois 1 versus le reste (0,2, ..., 9). Et ainsi de suite. Ce qui nous donnera 10 classifieurs, chacun répondant à la question \"est-ce que cette image est un 0? un 1? etc.\n",
    "- Etant donné une nouvelle image, on la passe dans les 10 classifieurs, et en fonction des 10 prédictions on prend une décision.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### b. Manipulation des données et construction du premier classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "from scipy.optimize import LinearConstraint, minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['ytick.left'] = True\n",
    "plt.rcParams['xtick.labelbottom'] = True\n",
    "plt.rcParams['ytick.labelleft'] = True\n",
    "plt.rcParams['image.cmap'] = 'gray_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = sklearn.datasets.load_digits() # Importe un jeu de données à classer (10 classes)\n",
    "digits.data = digits.data*1/np.max(digits.data) # normalise : coeffs dans [0,1]\n",
    "X, X_test, Y, Y_test = sklearn.model_selection.train_test_split( # On coupe le jeu de données en deux\n",
    "                        digits.data, digits.target, train_size=0.25, shuffle=True)\n",
    "etiquettes_original = Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Déterminer `m` le nombre de données contenues dans `X`, et `n` la dimension de ces données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[0]\n",
    "n = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Vérifier que `Y` contient bien des étiquettes allant de 0 à 9. Dans le but de classer les 0 vs. le reste, créez un nouveau vecteur d'étiquettes `Y_temp` qui vaut +1 pour les 0, et -1 pour le reste. Faites bien attention à ne pas modifier le `Y` original!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 vs le reste\n",
    "cible = 0\n",
    "Y_temp = np.where(etiquettes_original==cible, 1, -1) # Transforme 'cible' en +1, le reste en -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** En vous inspirant de ce que vous avez fait à la section précédente:\n",
    "\n",
    "- définissez le problème de SVM associé au problème de classifier 0 vs. le reste\n",
    "- résolvez-le, afin d'obtenir `w` un vecteur de paramètres définissant un hyperplan qui sépare les 0 des autres chiffres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** Prenez quelques examples dans `X` et vérifiez que `w` définit un hyperplan qui sépare bien les 0 du reste (cf. section précédente). Rappelez-vous que les vraies étiquettes sont contenues dans `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### c. Construction d'un classifieur général"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Ici vous devrez reproduire ce que vous venez de faire :\n",
    "\n",
    "- Pour chaque chiffre 'cible' entre 0 et 9:\n",
    "  - Définir un vecteur d'étiquettes décrivant un problème de classification 'cible' vs. le reste\n",
    "  - Définir le problème SVM associé et le résoudre, ce qui vous donnera un vecteurs de paramètres w\n",
    "  - Ranger chacun de ces vecteurs $w \\in \\mathbb{R}^{n+1}$ comme ligne d'une matrice $W \\in \\mathcal{M}_{10, n+1}(\\mathbb{R})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Vérifiez que la matrice `W` obtenue donne de bons résultats. Pour cela, pous pourrez prendre une image quelconque, et la passer dans le classifieur binaire pour chaque ligne de `W` : il devrait renvoyer +1 seulement pour le bon indice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Définir une fonction `classifieur_general` qui:\n",
    "\n",
    "- prend en entrée une donnée $x$ et une matrice de paramètres  $W \\in \\mathcal{M}_{10, n+1}(\\mathbb{R})$\n",
    "- pour chaque chiffre `i` entre 0 et 9, utilise le classifieur binaire induit par `W[i,:]` pour déterminer si $x$ est égal à `i` ou non\n",
    "- renvoie `i` si $x$ est égal à `i`\n",
    "- pensez à faire en sorte que la fonction renvoie toujours quelque chose\n",
    "\n",
    "Vous testerez sur un exemple que cela marche bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifieur_general(x, W):\n",
    "    # #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** Appliquez votre `classifieur_general` aux données de test, comme au **II.3.1)**. Quel est le pourcentage de bonnes réponses de votre classifieur? Que pensez-vous du résultat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### d. Construction d'un classifieur général amélioré\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le classifieur précédent souffre de quelques défauts.\n",
    "Par exemple, si une image est classée comme appartenant à deux classes, comment choisir laquelle des deux est la meilleure?\n",
    "Et si une image apparait classée comme appartenant à aucune classe, comment choisir quelle classe est la moins mauvaise?\n",
    "On donc besoin que les classifieurs binaires renvoient un peu plus que une réponse $\\pm 1$ pour chaque chiffre : il nous faut également un indice de confiance, une *probabilité* que la réponse $\\pm 1$ soit correcte.\n",
    "\n",
    "Par exemple, imaginons que le classifieur biniaire \"0 vs. le reste\" nous dise que $\\mathbb{P}(x=0) = 0.4$. Dans ce cas on pourrait conclure que $x$ n'est pas un 0, puisque $\\mathbb{P}(x\\neq 0) = 0.6$. Mais si tous les autres classifieurs binaires nous disent également que $\\mathbb{P}(x=k) = 0.01$, alors on pourrait se dire que 0 est la moins mauvaise réponse.\n",
    "\n",
    "Pour notre problème, cette probabilité va être liée à la distance entre la donnée $x$ et l'hyperplan séparateur $H_w$ : plus la donnée est proche de l'hyperplan, et moins on aura confiance en la prédiction, donc plus basse sera la probabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Définir une fonction `classifieur_general2` qui reprend le principe de `classifieur_general`. Cette fois-ci, l'indice `i` renvoyé sera celui qui maximise la probabilité $p_i$, où $p \\in \\mathbb{R}^{10}$ est défini par :\n",
    "\n",
    "$$ p_i := \\frac{e^{v_i}}{\\sum\\limits_{j=1}^p e^{v_j}},\n",
    "\\quad\n",
    "v_i := \\langle a^i, x \\rangle + b^i\n",
    "$$\n",
    "avec `W[i,:]`$=(a^i, b^i)$. \n",
    "Vous testerez sur un exemple que votre fonction marche bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Estimer la performance de votre nouveau classifieur sur les données de test. Comparer avec le classifieur précédent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour aller plus loin:** \n",
    "\n",
    "- Rien ne vous empêche de dessiner vos propres chiffres, et de tester si votre classifieur le reconnait.. Pour cela il vous suffit de produire une image carrée, de la redimensionner en image 8x8, puis de l'importer dans python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "Ressources utilisées pour le chargment et utilisation de MNIST:\n",
    "\n",
    "- https://dmkothari.github.io/Machine-Learning-Projects/SVM_with_MNIST.html\n",
    "- https://towardsdatascience.com/support-vector-machine-mnist-digit-classification-with-python-including-my-hand-written-digits-83d6eca7004a\n",
    "- https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
